{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbWoGOis4KoG"
   },
   "source": [
    "# Lab 5: Adding Structure to your Evaluations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will run experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to evaluate your agent example. Here are the evaluators you will use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from phoenix.evals import TOOL_CALLING_PROMPT_TEMPLATE, OpenAIModel, llm_classify\n",
    "from phoenix.experiments import evaluate_experiment, run_experiment\n",
    "from phoenix.experiments.evaluators import create_evaluator\n",
    "from phoenix.experiments.types import Example\n",
    "from phoenix.otel import register\n",
    "from utils import (\n",
    "    get_phoenix_endpoint,\n",
    "    process_messages,\n",
    "    run_agent,\n",
    "    tools,\n",
    "    update_sql_gen_prompt,\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset of Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = OpenAIModel(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_client = px.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_experiment_questions = [\n",
    "    {\n",
    "        \"question\": \"What was the most popular product SKU?\",\n",
    "        \"sql_result\": \"   SKU_Coded  Total_Qty_Sold 0    6200700         52262.0\",\n",
    "        \"sql_generated\": \"```sql\\nSELECT SKU_Coded, SUM(Qty_Sold) AS Total_Qty_Sold\\nFROM sales\\nGROUP BY SKU_Coded\\nORDER BY Total_Qty_Sold DESC\\nLIMIT 1;\\n```\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was the total revenue across all stores?\",\n",
    "        \"sql_result\": \"   Total_Revenue 0   1.327264e+07\",\n",
    "        \"sql_generated\": \"```sql\\nSELECT SUM(Total_Sale_Value) AS Total_Revenue\\nFROM sales;\\n```\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which store had the highest sales volume?\",\n",
    "        \"sql_result\": \"   Store_Number  Total_Sales_Volume 0          2970             59322.0\",\n",
    "        \"sql_generated\": \"```sql\\nSELECT Store_Number, SUM(Total_Sale_Value) AS Total_Sales_Volume\\nFROM sales\\nGROUP BY Store_Number\\nORDER BY Total_Sales_Volume DESC\\nLIMIT 1;\\n```\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Create a bar chart showing total sales by store\",\n",
    "        \"sql_result\": \"    Store_Number    Total_Sales 0            880  420302.088397 1           1650  580443.007953 2           4180  272208.118542 3            550  229727.498752 4           1100  497509.528013 5           3300  619660.167018 6           3190  335035.018792 7           2970  836341.327191 8           3740  359729.808228 9           2530  324046.518720 10          4400   95745.620250 11          1210  508393.767785 12           330  370503.687331 13          2750  453664.808068 14          1980  242290.828499 15          1760  350747.617798 16          3410  410567.848126 17           990  378433.018639 18          4730  239711.708869 19          4070  322307.968330 20          3080  495458.238811 21          2090  309996.247965 22          1320  592832.067579 23          2640  308990.318559 24          1540  427777.427815 25          4840  389056.668316 26          2860  132320.519487 27          2420  406715.767402 28           770  292968.918642 29          3520  145701.079372 30           660  343594.978075 31          3630  405034.547846 32          2310  412579.388504 33          2200  361173.288199 34          1870  401070.997685\",\n",
    "        \"sql_generated\": \"```sql\\nSELECT Store_Number, SUM(Total_Sale_Value) AS Total_Sales\\nFROM sales\\nGROUP BY Store_Number;\\n```\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What percentage of items were sold on promotion?\",\n",
    "        \"sql_result\": \"   Promotion_Percentage 0              0.625596\",\n",
    "        \"sql_generated\": \"```sql\\nSELECT \\n    (SUM(CASE WHEN On_Promo = 'Yes' THEN 1 ELSE 0 END) * 100.0) / COUNT(*) AS Promotion_Percentage\\nFROM \\n    sales;\\n```\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was the average transaction value?\",\n",
    "        \"sql_result\": \"   Average_Transaction_Value 0                  19.018132\",\n",
    "        \"sql_generated\": \"```sql\\nSELECT AVG(Total_Sale_Value) AS Average_Transaction_Value\\nFROM sales;\\n```\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Create a line chart showing sales in 2021\",\n",
    "        \"sql_result\": \"  sale_month  total_quantity_sold  total_sales_value 0 2021-11-01              43056.0      499984.428193 1 2021-12-01              75724.0      910982.118423\",\n",
    "        \"sql_generated\": \"```sql\\nSELECT MONTH(Sold_Date) AS Month, SUM(Total_Sale_Value) AS Total_Sales\\nFROM sales\\nWHERE YEAR(Sold_Date) = 2021\\nGROUP BY MONTH(Sold_Date)\\nORDER BY MONTH(Sold_Date);\\n```\",\n",
    "    },\n",
    "]\n",
    "\n",
    "overall_experiment_df = pd.DataFrame(overall_experiment_questions)\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# create a dataset consisting of input questions and expected outputs\n",
    "dataset = px_client.upload_dataset(\n",
    "    dataframe=overall_experiment_df,\n",
    "    dataset_name=f\"overall_experiment_inputs-{now}\",\n",
    "    input_keys=[\"question\"],\n",
    "    output_keys=[\"sql_result\", \"sql_generated\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to Phoenix UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open this link to check out the Phoenix UI and the uploaded dataset. You can use the same link to check out the results of the experiment you'll run in this notebook. \n",
    "\n",
    "**Note**: \n",
    "- Since each notebook of this course runs in an isolated environment, each notebook links to a different Phoenix server. This is why you won't see the projects you've worked on in the previous notebooks. \n",
    "- Make sure that the notebook's kernel is running when checking the Phoenix UI. If the link does not open, it might be because the notebook has been open or inactive for a long time. In that case, make sure to refresh the browser, run all previous cells and then check this link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_phoenix_endpoint())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the template prompts for the LLM-as-a-judge that will be used to evaluate the clarity of the analysis of tool 2 and the correctness of the entities mentioned in the analysis. For router evals, you will use the template provided by Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLARITY_LLM_JUDGE_PROMPT = \"\"\"\n",
    "In this task, you will be presented with a query and an answer. Your objective is to evaluate the clarity \n",
    "of the answer in addressing the query. A clear response is one that is precise, coherent, and directly \n",
    "addresses the query without introducing unnecessary complexity or ambiguity. An unclear response is one \n",
    "that is vague, disorganized, or difficult to understand, even if it may be factually correct.\n",
    "\n",
    "Your response should be a single word: either \"clear\" or \"unclear,\" and it should not include any other \n",
    "text or characters. \"clear\" indicates that the answer is well-structured, easy to understand, and \n",
    "appropriately addresses the query. \"unclear\" indicates that the answer is ambiguous, poorly organized, or \n",
    "not effectively communicated. Please carefully consider the query and answer before determining your \n",
    "response.\n",
    "\n",
    "After analyzing the query and the answer, you must write a detailed explanation of your reasoning to \n",
    "justify why you chose either \"clear\" or \"unclear.\" Avoid stating the final label at the beginning of your \n",
    "explanation. Your reasoning should include specific points about how the answer does or does not meet the \n",
    "criteria for clarity.\n",
    "\n",
    "[BEGIN DATA]\n",
    "Query: {query}\n",
    "Answer: {response}\n",
    "[END DATA]\n",
    "Please analyze the data carefully and provide an explanation followed by your response.\n",
    "\n",
    "EXPLANATION: Provide your reasoning step by step, evaluating the clarity of the answer based on the query.\n",
    "LABEL: \"clear\" or \"unclear\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_CORRECTNESS_LLM_JUDGE_PROMPT = \"\"\"\n",
    "In this task, you will be presented with a query and an answer. Your objective is to determine whether all \n",
    "the entities mentioned in the answer are correctly identified and accurately match those in the query. An \n",
    "entity refers to any specific person, place, organization, date, or other proper noun. Your evaluation \n",
    "should focus on whether the entities in the answer are correctly named and appropriately associated with \n",
    "the context in the query.\n",
    "\n",
    "Your response should be a single word: either \"correct\" or \"incorrect,\" and it should not include any \n",
    "other text or characters. \"correct\" indicates that all entities mentioned in the answer match those in the \n",
    "query and are properly identified. \"incorrect\" indicates that the answer contains errors or mismatches in \n",
    "the entities referenced compared to the query.\n",
    "\n",
    "After analyzing the query and the answer, you must write a detailed explanation of your reasoning to \n",
    "justify why you chose either \"correct\" or \"incorrect.\" Avoid stating the final label at the beginning of \n",
    "your explanation. Your reasoning should include specific points about how the entities in the answer do or \n",
    "do not match the entities in the query.\n",
    "\n",
    "[BEGIN DATA]\n",
    "Query: {query}\n",
    "Answer: {response}\n",
    "[END DATA]\n",
    "Please analyze the data carefully and provide an explanation followed by your response.\n",
    "\n",
    "EXPLANATION: Provide your reasoning step by step, evaluating whether the entities in the answer are \n",
    "correct and consistent with the query.\n",
    "LABEL: \"correct\" or \"incorrect\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following evaluators are set up to take in as parameters: input, output and expected. Here are the structures and meaning of these parameters:\n",
    "- **input**: this is the input field of your dataset examples that you just created. It has only one key: \"question\" (as defined in a previous cell).\n",
    "- **output**: this is the output field added to your dataset examples, after you apply the task to each example. The structure of this output is defined by the task, which is defined in a subsequent cell (as `run_agent_task`). This task returns a processed version of the agent's messages (you can check `process_messages` in `utils.py`): it is a dictionary that organizes the messages into these keys: \"tool_calls\", \"tool_responses\", \"final_output\", \"unchanged_messages\" and \"path_length\".\n",
    "- **expected**: this is the expected output field of your dataset examples that you created in a previous cell. It has two keys: \"sql_result\" and \"sql_generated\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator for the router\n",
    "def function_calling_eval(input: str, output: str) -> float:\n",
    "    if output is None:\n",
    "        return 0\n",
    "    function_calls = output.get(\"tool_calls\")\n",
    "    if function_calls:\n",
    "        eval_df = pd.DataFrame(\n",
    "            {\n",
    "                \"question\": [input.get(\"question\")] * len(function_calls),\n",
    "                \"tool_call\": function_calls,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        tool_call_eval = llm_classify(\n",
    "            data=eval_df,\n",
    "            template=TOOL_CALLING_PROMPT_TEMPLATE.template[0].template.replace(\n",
    "                \"{tool_definitions}\",\n",
    "                json.dumps(tools).replace(\"{\", '\"').replace(\"}\", '\"'),\n",
    "            ),\n",
    "            rails=[\"correct\", \"incorrect\"],\n",
    "            model=eval_model,\n",
    "            provide_explanation=True,\n",
    "        )\n",
    "\n",
    "        tool_call_eval[\"score\"] = tool_call_eval.apply(\n",
    "            lambda x: 1 if x[\"label\"] == \"correct\" else 0, axis=1\n",
    "        )\n",
    "        return tool_call_eval[\"score\"].mean()\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator for tool 1: database lookup\n",
    "def evaluate_sql_result(output, expected) -> bool:\n",
    "    if output is None:\n",
    "        return False\n",
    "    sql_result = output.get(\"tool_responses\")\n",
    "    if not sql_result:\n",
    "        return True\n",
    "\n",
    "    # Find first lookup_sales_data response\n",
    "    sql_result = next(\n",
    "        (r for r in sql_result if r.get(\"tool_name\") == \"lookup_sales_data\"), None\n",
    "    )\n",
    "    if not sql_result:\n",
    "        return True\n",
    "\n",
    "    # Get the first response\n",
    "    sql_result = sql_result.get(\"tool_response\", \"\")\n",
    "\n",
    "    # Extract just the numbers from both strings\n",
    "    result_nums = \"\".join(filter(str.isdigit, sql_result))\n",
    "    expected_nums = \"\".join(filter(str.isdigit, expected.get(\"sql_result\")))\n",
    "    return result_nums == expected_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator for tool 2: data analysis\n",
    "def evaluate_clarity(output: str, input: str) -> bool:\n",
    "    if output is None:\n",
    "        return False\n",
    "    df = pd.DataFrame(\n",
    "        {\"query\": [input.get(\"question\")], \"response\": [output.get(\"final_output\")]}\n",
    "    )\n",
    "    response = llm_classify(\n",
    "        data=df,\n",
    "        template=CLARITY_LLM_JUDGE_PROMPT,\n",
    "        rails=[\"clear\", \"unclear\"],\n",
    "        model=eval_model,\n",
    "        provide_explanation=True,\n",
    "    )\n",
    "    return response[\"label\"] == \"clear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator for tool 2: data analysis\n",
    "def evaluate_entity_correctness(output: str, input: str) -> bool:\n",
    "    if output is None:\n",
    "        return False\n",
    "    df = pd.DataFrame(\n",
    "        {\"query\": [input.get(\"question\")], \"response\": [output.get(\"final_output\")]}\n",
    "    )\n",
    "    response = llm_classify(\n",
    "        data=df,\n",
    "        template=ENTITY_CORRECTNESS_LLM_JUDGE_PROMPT,\n",
    "        rails=[\"correct\", \"incorrect\"],\n",
    "        model=eval_model,\n",
    "        provide_explanation=True,\n",
    "    )\n",
    "    return response[\"label\"] == \"correct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator for tool 3: data visualization\n",
    "def code_is_runnable(output: str) -> bool:\n",
    "    \"\"\"Check if the code is runnable\"\"\"\n",
    "    if output is None:\n",
    "        return False\n",
    "    generated_code = output.get(\"tool_responses\")\n",
    "    if not generated_code:\n",
    "        return True\n",
    "\n",
    "    # Find first lookup_sales_data response\n",
    "    generated_code = next(\n",
    "        (r for r in generated_code if r.get(\"tool_name\") == \"generate_visualization\"),\n",
    "        None,\n",
    "    )\n",
    "    if not generated_code:\n",
    "        return True\n",
    "\n",
    "    # Get the first response\n",
    "    generated_code = generated_code.get(\"tool_response\", \"\")\n",
    "    generated_code = generated_code.strip()\n",
    "    generated_code = generated_code.replace(\"```python\", \"\").replace(\"```\", \"\")\n",
    "    try:\n",
    "        exec(generated_code)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_task(example: Example) -> str:\n",
    "    print(\"Starting agent with messages:\", example.input.get(\"question\"))\n",
    "    messages = [{\"role\": \"user\", \"content\": example.input.get(\"question\")}]\n",
    "    ret = run_agent(messages)\n",
    "    return process_messages(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Experiment - Change in Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"\"\"\n",
    "Generate an SQL query based on a prompt. \n",
    "Do not reply with anything besides the SQL query.\n",
    "The prompt is: {prompt}\n",
    "\n",
    "The available columns are: {columns}\n",
    "The table name is: {table_name}\n",
    "\n",
    "Think before you respond.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_sql_gen_prompt(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = run_experiment(\n",
    "    dataset,\n",
    "    run_agent_task,\n",
    "    evaluators=[\n",
    "        function_calling_eval,\n",
    "        evaluate_sql_result,\n",
    "        evaluate_clarity,\n",
    "        evaluate_entity_correctness,\n",
    "        code_is_runnable,\n",
    "    ],\n",
    "    experiment_name=\"Overall Experiment v2\",\n",
    "    experiment_description=\"Evaluating the overall experiment, with changes to sql prompt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - Playground in Phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can try the playground feature in Phoenix UI to change the prompt and compare the results. You can run the following cells to get the prompt that you can input in the UI.\n",
    "\n",
    "**Note** that this is an optional part, and the OpenAI key is only provided in this notebook and not in the Phoenix server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_sql_gen_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_sql_gen_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
