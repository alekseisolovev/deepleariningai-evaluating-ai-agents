{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbWoGOis4KoG"
   },
   "source": [
    "# Lab 4: Adding Trajectory Evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will compute the convergence score of the agent, and you will use Phoenix experiments to set up this evaluation. The video goes through the components of an experiment (dataset consisting of examples, task, evaluator). \n",
    "\n",
    "Here's how you will define the experiment components in this lab to compute the convergence score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create a dataset of Phoenix examples where each example contains a different version of the same question. You will then create the task `run_agent_and_track_path` that you will use to run on each example and compute the path length. Finally, you will create the evaluator `evaluate_path_length` that computes the convergence score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from phoenix.evals import OpenAIModel\n",
    "from phoenix.experiments import evaluate_experiment, run_experiment\n",
    "from phoenix.experiments.evaluators import create_evaluator\n",
    "from phoenix.experiments.types import Example\n",
    "from phoenix.otel import register\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_phoenix_endpoint, run_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset of Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_client = px.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_questions = [\n",
    "    \"What was the average quantity sold per transaction?\",\n",
    "    \"What is the mean number of items per sale?\",\n",
    "    \"Calculate the typical quantity per transaction\",\n",
    "    \"What's the mean transaction size in terms of quantity?\",\n",
    "    \"On average, how many items were purchased per transaction?\",\n",
    "    \"What is the average basket size per sale?\",\n",
    "    \"Calculate the mean number of products per purchase\",\n",
    "    \"What's the typical number of units per order?\",\n",
    "    \"What is the average number of products bought per purchase?\",\n",
    "    \"Tell me the mean quantity of items in a typical transaction\",\n",
    "    \"How many items does a customer buy on average per transaction?\",\n",
    "    \"What's the usual number of units in each sale?\",\n",
    "    \"What is the typical amount of products per transaction?\",\n",
    "    \"Show the mean number of items customers purchase per visit\",\n",
    "    \"What's the average quantity of units per shopping trip?\",\n",
    "    \"How many products do customers typically buy in one transaction?\",\n",
    "    \"What is the standard basket size in terms of quantity?\",\n",
    "]\n",
    "\n",
    "convergence_df = pd.DataFrame({\"question\": convergence_questions})\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "dataset = px_client.upload_dataset(\n",
    "    dataframe=convergence_df,\n",
    "    dataset_name=f\"convergence_questions-{now}\",\n",
    "    input_keys=[\"question\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to Phoenix UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open this link to check out the Phoenix UI and the uploaded dataset. You can use the same link to check out the results of the experiment you'll run in this notebook. \n",
    "\n",
    "**Note**: \n",
    "- Since each notebook of this course runs in an isolated environment, each notebook links to a different Phoenix server. This is why you won't see the projects you've worked on in the previous notebooks. \n",
    "- Make sure that the notebook's kernel is running when checking the Phoenix UI. If the link does not open, it might be because the notebook has been open or inactive for a long time. In that case, make sure to refresh the browser, run all previous cells and then check this link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_phoenix_endpoint())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to format the output returned by the task\n",
    "def format_message_steps(messages):\n",
    "    \"\"\"\n",
    "    Convert a list of message objects into a readable format that shows the steps taken.\n",
    "\n",
    "    Args:\n",
    "        messages (list): A list of message objects containing role, content, tool calls, etc.\n",
    "\n",
    "    Returns:\n",
    "        str: A readable string showing the steps taken.\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    for message in messages:\n",
    "        role = message.get(\"role\")\n",
    "        if role == \"user\":\n",
    "            steps.append(f\"User: {message.get('content')}\")\n",
    "        elif role == \"system\":\n",
    "            steps.append(\"System: Provided context\")\n",
    "        elif role == \"assistant\":\n",
    "            if message.get(\"tool_calls\"):\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    tool_name = tool_call[\"function\"][\"name\"]\n",
    "                    steps.append(f\"Assistant: Called tool '{tool_name}'\")\n",
    "            else:\n",
    "                steps.append(f\"Assistant: {message.get('content')}\")\n",
    "        elif role == \"tool\":\n",
    "            steps.append(f\"Tool response: {message.get('content')}\")\n",
    "\n",
    "    return \"\\n\".join(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_and_track_path(example: Example) -> str:\n",
    "    messages = [{\"role\": \"user\", \"content\": example.input.get(\"question\")}]\n",
    "    ret = run_agent(messages)\n",
    "    return {\"path_length\": len(ret), \"messages\": format_message_steps(ret)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = run_experiment(\n",
    "    dataset,\n",
    "    run_agent_and_track_path,\n",
    "    experiment_name=\"Convergence Eval\",\n",
    "    experiment_description=\"Evaluating the convergence of the agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = experiment.as_dataframe()[\"output\"].to_dict().values()\n",
    "\n",
    "# Will include the user and system messages\n",
    "optimal_path_length = min(\n",
    "    output.get(\"path_length\")\n",
    "    for output in outputs\n",
    "    if output and output.get(\"path_length\") is not None\n",
    ")\n",
    "print(f\"The optimal path length is {optimal_path_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@create_evaluator(name=\"Convergence Eval\", kind=\"CODE\")\n",
    "def evaluate_path_length(output: str) -> float:\n",
    "    if output and output.get(\"path_length\"):\n",
    "        return optimal_path_length / float(output.get(\"path_length\"))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = evaluate_experiment(experiment, evaluators=[evaluate_path_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
